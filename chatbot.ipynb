{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "model_dir = \"bert\"\n",
    "\n",
    "config_directory = \"bert/config.json\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "model = BertModel.from_pretrained(model_dir)\n",
    "\n",
    "config = BertConfig.from_json_file(config_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "PDF = \"contract.txt\"\n",
    "loader = TextLoader(PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'contract.txt'}, page_content=\"Contract for Catering Services\\nThe starting date of the contract is June 24, 2024.\\nThe ending date of the contract is June 24, 2025.\\nThe employer of the contract is HealthCare Solutions.\\nThe vendor of the contract is Gourmet Delights Catering.\\nThe department involved in the contract is the Events Department.\\nThe service provided by Gourmet Delights Catering is menu planning for corporate events.\\nThe service provided by Gourmet Delights Catering will prepare and serve food for corporate events.\\nThe service provided by Gourmet Delights Catering will set up and clean up after corporate events.\\nThe service provided by Gourmet Delights Catering will provide beverage service for corporate events.\\nThe payment details are that HealthCare Solutions will pay Gourmet Delights Catering $10,000 per event.\\nThe payment details are that Payments must be made within 10 days after each event.\\nThe terms and conditions are that Gourmet Delights Catering will keep the event details confidential.\\nThe terms and conditions are that Either party can terminate the contract with 30 days' written notice.\\nThe terms and conditions are that All food will be prepared according to health regulations.\\nSignatures\\nHealthCare Solutions\\nGourmet Delights Catering\")]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class SentenceTextSplitter:\n",
    "    def __init__(self, chunk_size, chunk_overlap):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_text(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            if current_length + sentence_length > self.chunk_size:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = current_chunk[-self.chunk_overlap:]\n",
    "                current_length = sum(len(sent) for sent in current_chunk)\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "splitter = SentenceTextSplitter(chunk_size=100, chunk_overlap=30)\n",
    "chunks = splitter.split_text(output[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=30,separators=[\"\\n\\n\",\"\\n\", \".\"])\n",
    "splitted_text = text_splitter.split_documents(documents=output)\n",
    "print(len(splitted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'contract.txt'}, page_content='Contract for Catering Services\\nThe starting date of the contract is June 24, 2024.'), Document(metadata={'source': 'contract.txt'}, page_content='The ending date of the contract is June 24, 2025.'), Document(metadata={'source': 'contract.txt'}, page_content='The employer of the contract is HealthCare Solutions.'), Document(metadata={'source': 'contract.txt'}, page_content='The vendor of the contract is Gourmet Delights Catering.'), Document(metadata={'source': 'contract.txt'}, page_content='The department involved in the contract is the Events Department.'), Document(metadata={'source': 'contract.txt'}, page_content='The service provided by Gourmet Delights Catering is menu planning for corporate events.'), Document(metadata={'source': 'contract.txt'}, page_content='The service provided by Gourmet Delights Catering will prepare and serve food for corporate events.'), Document(metadata={'source': 'contract.txt'}, page_content='The service provided by Gourmet Delights Catering will set up and clean up after corporate events.'), Document(metadata={'source': 'contract.txt'}, page_content='\\nThe service provided by Gourmet Delights Catering will provide beverage service for corporate events'), Document(metadata={'source': 'contract.txt'}, page_content='.'), Document(metadata={'source': 'contract.txt'}, page_content='\\nThe payment details are that HealthCare Solutions will pay Gourmet Delights Catering $10,000 per event'), Document(metadata={'source': 'contract.txt'}, page_content='.'), Document(metadata={'source': 'contract.txt'}, page_content='The payment details are that Payments must be made within 10 days after each event.'), Document(metadata={'source': 'contract.txt'}, page_content='\\nThe terms and conditions are that Gourmet Delights Catering will keep the event details confidential'), Document(metadata={'source': 'contract.txt'}, page_content='.'), Document(metadata={'source': 'contract.txt'}, page_content=\"\\nThe terms and conditions are that Either party can terminate the contract with 30 days' written notice\"), Document(metadata={'source': 'contract.txt'}, page_content='.'), Document(metadata={'source': 'contract.txt'}, page_content='The terms and conditions are that All food will be prepared according to health regulations.'), Document(metadata={'source': 'contract.txt'}, page_content='Signatures\\nHealthCare Solutions\\nGourmet Delights Catering')]\n"
     ]
    }
   ],
   "source": [
    "print(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Function to compute embeddings using BertModel\n",
    "def compute_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    embeddings = torch.mean(last_hidden_state, dim=1).squeeze().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "def store_embeddings_in_faiss(splitted_text, index_filename):\n",
    "    embedding_dim = 768  # Bert base model output size\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    \n",
    "    docstore = {}\n",
    "    index_to_docstore_id = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for chunk in splitted_text:\n",
    "        text = chunk.page_content\n",
    "        embeddings = compute_bert_embeddings(text)\n",
    "        embeddings = np.array(embeddings, dtype=np.float32).reshape(1, -1)\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # Assuming chunk has a unique identifier like doc_id\n",
    "        doc_id = f\"doc_{idx}\"\n",
    "        docstore[doc_id] = {\n",
    "            'text': text,\n",
    "            'source': chunk.metadata['source']\n",
    "        }\n",
    "        index_to_docstore_id[idx] = doc_id\n",
    "        idx += 1\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, index_filename + \".faiss\")\n",
    "    \n",
    "    # Save associated metadata (docstore and index_to_docstore_id)\n",
    "    metadata = {\n",
    "        'docstore': docstore,\n",
    "        'index_to_docstore_id': index_to_docstore_id\n",
    "    }\n",
    "    with open(index_filename + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_filename = \"index\"\n",
    "store_embeddings_in_faiss(splitted_text, index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<faiss.swigfaiss.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x00000212CA2032A0> >\n"
     ]
    }
   ],
   "source": [
    "vector_store_directory = \"index.faiss\"\n",
    "vector_store = faiss.read_index(vector_store_directory)\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever:\n",
    "    def __init__(self, index_filename):\n",
    "        self.index = faiss.read_index(index_filename)\n",
    "\n",
    "    def retrieve_documents(self, query, top_k=3):\n",
    "        # Implement your retrieval logic here\n",
    "        # Example using FAISS:\n",
    "        embeddings = compute_bert_embeddings(query)  # Implement compute_bert_embeddings\n",
    "        embeddings = np.array(embeddings, dtype=np.float32).reshape(1, -1)\n",
    "        distances, indices = self.index.search(embeddings, top_k)\n",
    "        print(distances, indices)\n",
    "        reversed_indices = indices[0][::-1] \n",
    "        print(reversed_indices)\n",
    "        reversed_indices_2d = np.expand_dims(reversed_indices, axis=0)\n",
    "        return reversed_indices_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "\n",
    "qna_tokenizer = AutoTokenizer.from_pretrained(\"./bert-for-qna\")\n",
    "qna_model = BertForQuestionAnswering.from_pretrained(\"./bert-for-qna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Generator:\n",
    "    def __init__(self, tokenizer, model, device=\"cpu\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, retrieved_documents, query):\n",
    "        contexts = [doc.page_content for doc in retrieved_documents]\n",
    "        combined_context = \" \".join(contexts)\n",
    "\n",
    "        combined_context = re.sub(r'\\n\\s*\\n', '\\n\\n', combined_context).strip()\n",
    "\n",
    "        question = query\n",
    "        text = combined_context\n",
    "        \n",
    "        print(text)\n",
    "        inputs = self.tokenizer(question, text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Get the predicted start and end indices for the answer\n",
    "        answer_start_index = outputs.start_logits.argmax()\n",
    "        answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "        # Extract the tokens corresponding to the predicted answer span\n",
    "        predicted_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "        # Decode the tokens to get the predicted answer text\n",
    "        predicted_answer = self.tokenizer.decode(predicted_answer_tokens, skip_special_tokens=True)\n",
    "        print(\"Predicted Answer:\", predicted_answer)\n",
    "\n",
    "        # Define the target answer indices (for loss calculation)\n",
    "        target_start_index = torch.tensor([14])  # Modify as per your target answer start index\n",
    "        target_end_index = torch.tensor([15])    # Modify as per your target answer end index\n",
    "\n",
    "        # Compute the outputs including the loss\n",
    "        outputs = self.model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", round(loss.item(), 2))\n",
    "\n",
    "        # For seeing the actual target text\n",
    "        target_answer_tokens = inputs.input_ids[0, target_start_index : target_end_index + 1]\n",
    "        target_answer = self.tokenizer.decode(target_answer_tokens, skip_special_tokens=True)\n",
    "        print(\"Target Answer:\", target_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59.90132  64.599075 65.593506]] [[ 0  2 12]]\n",
      "[12  2  0]\n",
      "Top-k document indices: [[12  2  0]]\n"
     ]
    }
   ],
   "source": [
    "index_filename = \"index.faiss\"\n",
    "custom_retriever = CustomRetriever(index_filename)\n",
    "query = \"When is the contract starting?\"\n",
    "top_k_docs = custom_retriever.retrieve_documents(query)\n",
    "print(\"Top-k document indices:\", top_k_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'contract.txt'}, page_content='The payment details are that Payments must be made within 10 days after each event.'), Document(metadata={'source': 'contract.txt'}, page_content='The employer of the contract is HealthCare Solutions.'), Document(metadata={'source': 'contract.txt'}, page_content='Contract for Catering Services\\nThe starting date of the contract is June 24, 2024.')]\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents = [splitted_text[i] for i in top_k_docs[0]]\n",
    "print(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(qna_tokenizer,qna_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The payment details are that Payments must be made within 10 days after each event. The employer of the contract is HealthCare Solutions. Contract for Catering Services\n",
      "The starting date of the contract is June 24, 2024.\n",
      "Predicted Answer: June 24, 2024\n",
      "Loss: 15.68\n",
      "Target Answer: ##yment\n"
     ]
    }
   ],
   "source": [
    "generator.generate(retrieved_documents=retrieved_documents,query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
